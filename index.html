<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Machine Learning Cheat Sheet</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
      background-color: #f9f9f9;
    }
    h1, h2, h3 {
      color: #2c3e50;
    }
    code {
      background-color: #f4f4f4;
      padding: 2px 4px;
      font-size: 90%;
      border-radius: 4px;
    }
  </style>
</head>
<body>
  <h1>ğŸ“˜ Machine Learning Cheat Sheet</h1>

  <h2>1. Types of Learning</h2>
  <ul>
    <li><strong>Supervised Learning:</strong> Learn from labeled data to predict outputs.</li>
    <li><strong>Unsupervised Learning:</strong> Find patterns in data without labels.</li>
    <li><strong>Reinforcement Learning:</strong> Agent learns by interacting with environment via rewards.</li>
  </ul>

  <h2>2. Probability & Bayes Review</h2>
  <p>Used for modeling uncertainty and building probabilistic models.</p>
  <pre><code>Bayes Theorem:
P(Y | X) = P(X | Y) * P(Y) / P(X)
MLE:
Î¸_MLE = argmax_Î¸ P(D | Î¸)
MAP:
Î¸_MAP = argmax_Î¸ P(D | Î¸) * P(Î¸)</code></pre>

  <h2>3. Linear & Logistic Regression</h2>
  <p>Predict numerical values (Linear) or probabilities (Logistic) using weighted input features.</p>
  <pre><code>Linear: y = Xw + b
Logistic: P(y=1|x) = 1 / (1 + exp(-wáµ€x))</code></pre>

  <h2>4. Loss Functions & Gradient Descent</h2>
  <p>Used to measure error and update model weights to reduce error.</p>
  <pre><code>Square Loss: (y - Å·)Â²
Log Loss: -[y log(Å·) + (1 - y) log(1 - Å·)]
Gradient Descent: w â† w - Î· * âˆ‡L(w)</code></pre>

  <h2>5. Perceptron</h2>
  <p>Simple linear classifier that updates weights based on misclassifications.</p>
  <pre><code>Prediction: Å· = sign(wáµ€x + b)
Update: w â† w + yx, b â† b + y</code></pre>

  <h2>6. Support Vector Machines (SVM)</h2>
  <p>Finds a hyperplane that maximizes margin between two classes.</p>
  <pre><code>Hard-Margin: minimize Â½ ||w||Â² subject to yáµ¢(wáµ€xáµ¢ + b) â‰¥ 1
Soft-Margin: minimize Â½ ||w||Â² + C âˆ‘ Î¾áµ¢</code></pre>

  <h2>7. Kernel Methods</h2>
  <p>Allows linear algorithms to learn non-linear boundaries using kernel functions.</p>
  <pre><code>K(x, z) = Ï†(x)áµ€ Ï†(z)
RBF: exp(-Î³ ||x - z||Â²)</code></pre>

  <h2>8. Naive Bayes</h2>
  <p>Probabilistic classifier assuming feature independence given class.</p>
  <pre><code>P(x|y) = âˆ P(xâ±¼ | y)
y* = argmax_y P(y) âˆ P(xâ±¼ | y)</code></pre>

  <h2>9. K-Nearest Neighbors (KNN)</h2>
  <p>Classify based on majority label among k-nearest points.</p>
  <pre><code>Euclidean: sqrt(âˆ‘ (xáµ¢ - xáµ¢')Â²)
Manhattan: âˆ‘ |xáµ¢ - xáµ¢'|</code></pre>

  <h2>10. K-Means Clustering</h2>
  <p>Partition data into k clusters based on similarity (distance to centroids).</p>
  <pre><code>J = âˆ‘â‚– âˆ‘_{x âˆˆ Câ‚–} ||x - Î¼â‚–||Â²</code></pre>

  <h2>11. Expectation Maximization (EM)</h2>
  <p>Iterative algorithm for parameter estimation in latent variable models like Gaussian Mixtures.</p>
  <ul>
    <li><strong>E-step:</strong> Estimate responsibility</li>
    <li><strong>M-step:</strong> Maximize likelihood using current responsibilities</li>
  </ul>
  <pre><code>log L(Î¸) = âˆ‘_i log âˆ‘_z P(xáµ¢, z | Î¸)</code></pre>

  <h2>12. Decision Trees</h2>
  <p>Tree-structured model splitting features to make decisions with highest information gain.</p>
  <pre><code>Entropy: H(S) = -âˆ‘ páµ¢ logâ‚‚(páµ¢)
Information Gain: IG(S, A) = H(S) - âˆ‘_v (|Sáµ¥| / |S|) H(Sáµ¥)</code></pre>

  <h2>13. Boosting & AdaBoost</h2>
  <p>Combine weak learners iteratively, focusing on misclassified points to build strong ensemble.</p>
  <pre><code>Final hypothesis: H(x) = sign(âˆ‘â‚œ Î±â‚œ hâ‚œ(x))</code></pre>

  <h2>â­ Bias-Variance Tradeoff</h2>
  <p>Describes the balance between underfitting and overfitting.</p>
  <pre><code>Error = BiasÂ² + Variance + Irreducible Error</code></pre>

  <h2>âœ… Tips</h2>
  <ul>
    <li>Normalize features for KNN, SVM, SGD</li>
    <li>Use regularization to reduce overfitting</li>
    <li>Understand the kernel trick in SVM</li>
  </ul>
</body>
</html>
