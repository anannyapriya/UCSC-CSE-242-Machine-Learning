<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Machine Learning Cheat Sheet</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
      background-color: #f9f9f9;
    }
    h1, h2, h3 {
      color: #2c3e50;
    }
    code {
      background-color: #f4f4f4;
      padding: 2px 4px;
      font-size: 90%;
      border-radius: 4px;
    }
  </style>
</head>
<body>
  <h1>📘 Machine Learning Cheat Sheet</h1>

  <h2>1. Types of Learning</h2>
  <ul>
    <li><strong>Supervised Learning:</strong> Learn from labeled data to predict outputs.</li>
    <li><strong>Unsupervised Learning:</strong> Find patterns in data without labels.</li>
    <li><strong>Reinforcement Learning:</strong> Agent learns by interacting with environment via rewards.</li>
  </ul>

  <h2>2. Probability & Bayes Review</h2>
  <p>Used for modeling uncertainty and building probabilistic models.</p>
  <pre><code>Bayes Theorem:
P(Y | X) = P(X | Y) * P(Y) / P(X)
MLE:
θ_MLE = argmax_θ P(D | θ)
MAP:
θ_MAP = argmax_θ P(D | θ) * P(θ)</code></pre>

  <h2>3. Linear & Logistic Regression</h2>
  <p>Predict numerical values (Linear) or probabilities (Logistic) using weighted input features.</p>
  <pre><code>Linear: y = Xw + b
Logistic: P(y=1|x) = 1 / (1 + exp(-wᵀx))</code></pre>

  <h2>4. Loss Functions & Gradient Descent</h2>
  <p>Used to measure error and update model weights to reduce error.</p>
  <pre><code>Square Loss: (y - ŷ)²
Log Loss: -[y log(ŷ) + (1 - y) log(1 - ŷ)]
Gradient Descent: w ← w - η * ∇L(w)</code></pre>

  <h2>5. Perceptron</h2>
  <p>Simple linear classifier that updates weights based on misclassifications.</p>
  <pre><code>Prediction: ŷ = sign(wᵀx + b)
Update: w ← w + yx, b ← b + y</code></pre>

  <h2>6. Support Vector Machines (SVM)</h2>
  <p>Finds a hyperplane that maximizes margin between two classes.</p>
  <pre><code>Hard-Margin: minimize ½ ||w||² subject to yᵢ(wᵀxᵢ + b) ≥ 1
Soft-Margin: minimize ½ ||w||² + C ∑ ξᵢ</code></pre>

  <h2>7. Kernel Methods</h2>
  <p>Allows linear algorithms to learn non-linear boundaries using kernel functions.</p>
  <pre><code>K(x, z) = φ(x)ᵀ φ(z)
RBF: exp(-γ ||x - z||²)</code></pre>

  <h2>8. Naive Bayes</h2>
  <p>Probabilistic classifier assuming feature independence given class.</p>
  <pre><code>P(x|y) = ∏ P(xⱼ | y)
y* = argmax_y P(y) ∏ P(xⱼ | y)</code></pre>

  <h2>9. K-Nearest Neighbors (KNN)</h2>
  <p>Classify based on majority label among k-nearest points.</p>
  <pre><code>Euclidean: sqrt(∑ (xᵢ - xᵢ')²)
Manhattan: ∑ |xᵢ - xᵢ'|</code></pre>

  <h2>10. K-Means Clustering</h2>
  <p>Partition data into k clusters based on similarity (distance to centroids).</p>
  <pre><code>J = ∑ₖ ∑_{x ∈ Cₖ} ||x - μₖ||²</code></pre>

  <h2>11. Expectation Maximization (EM)</h2>
  <p>Iterative algorithm for parameter estimation in latent variable models like Gaussian Mixtures.</p>
  <ul>
    <li><strong>E-step:</strong> Estimate responsibility</li>
    <li><strong>M-step:</strong> Maximize likelihood using current responsibilities</li>
  </ul>
  <pre><code>log L(θ) = ∑_i log ∑_z P(xᵢ, z | θ)</code></pre>

  <h2>12. Decision Trees</h2>
  <p>Tree-structured model splitting features to make decisions with highest information gain.</p>
  <pre><code>Entropy: H(S) = -∑ pᵢ log₂(pᵢ)
Information Gain: IG(S, A) = H(S) - ∑_v (|Sᵥ| / |S|) H(Sᵥ)</code></pre>

  <h2>13. Boosting & AdaBoost</h2>
  <p>Combine weak learners iteratively, focusing on misclassified points to build strong ensemble.</p>
  <pre><code>Final hypothesis: H(x) = sign(∑ₜ αₜ hₜ(x))</code></pre>

  <h2>⭐ Bias-Variance Tradeoff</h2>
  <p>Describes the balance between underfitting and overfitting.</p>
  <pre><code>Error = Bias² + Variance + Irreducible Error</code></pre>

  <h2>✅ Tips</h2>
  <ul>
    <li>Normalize features for KNN, SVM, SGD</li>
    <li>Use regularization to reduce overfitting</li>
    <li>Understand the kernel trick in SVM</li>
  </ul>
</body>
</html>
